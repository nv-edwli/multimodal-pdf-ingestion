{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1faede4-d589-440d-9091-11477f9556e3",
   "metadata": {},
   "source": [
    "# Multi-Modal on PDFs with tables\n",
    "\n",
    "One common challenge with RAG (``Retrieval-Augmented Generation``) involves handling PDFs that contain tables. Parsing tables in various formats can be quite complex.\n",
    "\n",
    "However, Microsoftâ€™s newly released model, ``Table Transformer``, offers a promising solution for detecting tables within images.\n",
    "\n",
    "In this notebook, we will demonstrate how to leverage the ``Table Transformer`` model in conjunction with GPT4-V to yield better results for images containing tables.\n",
    "\n",
    "The experiment is divided into the following parts and we compared those 4 options for extracting table information from PDFs:\n",
    "\n",
    "Retrieving relevant images (PDF pages) and sending them to GPT4-V to respond to queries.\n",
    "\n",
    "Regarding every PDF page as an image, let GPT4-V do the image reasoning for each page. Build Text Vector Store index for the image reasonings. Query the answer against the ``Image Reasoning Vectore Store``.\n",
    "\n",
    "Using ``Table Transformer`` to crop the table information from the retrieved images and then sending these cropped images to GPT4-V for query responses.\n",
    "\n",
    "Applying OCR on cropped table images and send the data to GPT4/ GPT-3.5 to answer the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1045d-0709-4a53-82d4-63e26250df82",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93196745-4d8d-4819-807d-22c598a8300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-llms-openai\n",
    "# !pip install llama-index-multi-modal-llms-openai\n",
    "# !pip install llama-index-vector-stores-qdrant\n",
    "# !pip install llama-index-embeddings-clip\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install llama-index\n",
    "# !pip install qdrant_client\n",
    "# !pip install pyMuPDF\n",
    "# !pip install tools\n",
    "# !pip install frontend\n",
    "# !pip install easyocr\n",
    "# !pip install matplotlib\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d73ba8-0bf3-4f4a-83fb-23fd9d57ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Patch\n",
    "import io\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import AutoModelForObjectDetection\n",
    "import torch\n",
    "import openai\n",
    "import os\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "OPENAI_API_TOKEN = os.environ[\"OPENAI_API_TOKEN\"]\n",
    "openai.api_key = OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e80346-ea8f-460a-910d-58c734c3c50d",
   "metadata": {},
   "source": [
    "Download Llama2 paper for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49234625-cd61-4303-aa80-fcc7cc8b577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a6da53-9e9a-43dd-9bf8-18d959d460b5",
   "metadata": {},
   "source": [
    "Here we convert each of the Llama2 paper pdf page to images for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7e417-12ce-4865-a872-1caa37154a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"llama2.pdf\"\n",
    "\n",
    "# Split the base name and extension\n",
    "output_directory_path, _ = os.path.splitext(pdf_file)\n",
    "\n",
    "if not os.path.exists(output_directory_path):\n",
    "    os.makedirs(output_directory_path)\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open(pdf_file)\n",
    "\n",
    "# Iterate through each page and convert to an image\n",
    "for page_number in range(pdf_document.page_count):\n",
    "    # Get the page\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Convert the page to an image\n",
    "    pix = page.get_pixmap()\n",
    "\n",
    "    # Create a Pillow Image object from the pixmap\n",
    "    image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(f\"./{output_directory_path}/page_{page_number + 1}.png\")\n",
    "\n",
    "# Close the PDF file\n",
    "pdf_document.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a9c9c-f4fe-4b25-bee1-c12c30973cc3",
   "metadata": {},
   "source": [
    "Display the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ee0f6-2f39-4d9d-b69c-eeead389715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "image_paths = []\n",
    "for img_path in os.listdir(\"./llama2\"):\n",
    "    image_paths.append(str(os.path.join(\"./llama2\", img_path)))\n",
    "\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(3, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break\n",
    "\n",
    "\n",
    "plot_images(image_paths[9:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39149dbf-37fe-4151-beef-41efab02d343",
   "metadata": {},
   "source": [
    "# Experiment-1: Retrieving relevant images (PDF pages) and sending them to GPT4-V to respond to queries.\n",
    "\n",
    "We will now index the images with ``qdrant`` vector store using our ``MultiModalVectorStoreIndex`` abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af25b9-b787-49e6-a80c-ea17d4a0aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.core.schema import ImageDocument\n",
    "\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81fcd3-ba29-4949-997f-2105e2461d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "openai_mm_llm = OpenAIMultiModal(\n",
    "    model=\"gpt-4o\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04baaf34-c608-4a3d-b9a5-39e4ede04e67",
   "metadata": {},
   "source": [
    "Build the Multi-Modal retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f205951-3e0a-4477-acba-6c5063b0a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the images\n",
    "documents_images = SimpleDirectoryReader(\"./llama2/\").load_data()\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(path=\"qdrant_index\")\n",
    "\n",
    "text_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"text_collection\"\n",
    ")\n",
    "image_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"image_collection\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents_images,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "retriever_engine = index.as_retriever(image_similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253b069-ef2a-49a5-b748-8dc7593d144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.multi_modal.retriever import (\n",
    "    MultiModalVectorIndexRetriever,\n",
    ")\n",
    "\n",
    "query = \"Compare llama2 with llama1?\"\n",
    "assert isinstance(retriever_engine, MultiModalVectorIndexRetriever)\n",
    "# retrieve for the query using text to image retrieval\n",
    "retrieval_results = retriever_engine.text_to_image_retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb1472-17f4-49fa-963a-8667ce708250",
   "metadata": {},
   "source": [
    "Check the retrieved results from Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba542e3-6b28-4e7d-a622-b778c38572dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_images = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_images.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "plot_images(retrieved_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701a708-43dd-4786-8202-19dc45210cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd2d2a-df12-4977-bc21-36e8aa5d7a6a",
   "metadata": {},
   "source": [
    "Now letâ€™s send the retrieved images to GPT4-V for image reasoning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e2793-2e38-4817-b753-6d5f7c3b0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_documents = [\n",
    "    ImageDocument(image_path=image_path) for image_path in retrieved_images\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f860382-b9a6-4c3c-a2c7-419a2ef5459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_mm_llm.complete(\n",
    "    prompt=\"Compare llama2 with llama1?\",\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81804db0-6510-4626-bc11-db387b4da215",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "As you can see even though there is answer in the images, it is unable to give correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55989be-c7c8-4150-a9db-4f28b0c73126",
   "metadata": {},
   "source": [
    "# Experiment-2: Parse each pdf page as an image and get table date directly from GPT4-V. Index tables data and then do text retrieval\n",
    "\n",
    "Steps:\n",
    "\n",
    "* Extract and separate each PDF page as an image document\n",
    "\n",
    "* Let GPT4V identify table and extract table information from each PDF page\n",
    "\n",
    "* Index GPT4V understandings for each page into Image Reasoning Vector Store\n",
    "\n",
    "* Retrieve answer from this Image Reasoning Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d86413-6e54-4d36-b932-b7a0de5abdd1",
   "metadata": {},
   "source": [
    "## Load each pdf page as a image document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a23c74-b279-4b3a-b833-35f3e9f1fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# put your local directore here\n",
    "documents_images_v2 = SimpleDirectoryReader(\"./llama2/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f0a86-f586-4d91-b6b8-a1450f9b0f67",
   "metadata": {},
   "source": [
    "## Select one Image for Showcase the GPT4-V Table Reasoning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cf61d-705f-489b-899f-4d166a30e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(documents_images_v2[15].image_path).convert(\"RGB\")\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92d9ad-57c0-4390-b2b5-d6becd8ac4ad",
   "metadata": {},
   "source": [
    "## Using this one Image of PDF file for GPT4-V understanding as an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554abada-4419-4c0d-a5b4-5c0c44406e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_mm_llm = OpenAIMultiModal(\n",
    "    model=\"gpt-4o\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500\n",
    ")\n",
    "\n",
    "image_prompt = \"\"\"\n",
    "    Please load the table data and output in the json format from the image.\n",
    "    Please try your best to extract the table data from the image.\n",
    "    If you can't extract the table data, please summarize image and return the summary.\n",
    "\"\"\"\n",
    "response = openai_mm_llm.complete(\n",
    "    prompt=image_prompt,\n",
    "    image_documents=[documents_images_v2[15]],\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc944b28-3b0a-47b5-a38d-3cfab3675496",
   "metadata": {},
   "source": [
    "## Reuse the same prompt for all the pages in the PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368aed45-e1e1-4038-8ffa-1c2a12b84daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_results = {}\n",
    "for img_doc in documents_images_v2:\n",
    "    try:\n",
    "        image_table_result = openai_mm_llm.complete(\n",
    "            prompt=image_prompt,\n",
    "            image_documents=[img_doc],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error understanding for image {img_doc.image_path} from GPT4V API\"\n",
    "        )\n",
    "        continue\n",
    "    # image_results.append((image_document.image_path, image_table_result))\n",
    "    image_results[img_doc.image_path] = image_table_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471da735-9c33-45ff-9a48-20e8a43d3ebe",
   "metadata": {},
   "source": [
    "## Build Text-Only Vector Store by Indexing the Image Understandings from GPT4-V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819ccf0-424e-4bcb-b6db-0a8b813ce4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "text_docs = [\n",
    "    Document(\n",
    "        text=str(image_results[image_path]),\n",
    "        metadata={\"image_path\": image_path},\n",
    "    )\n",
    "    for image_path in image_results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f1e9d-aa38-40ee-9886-581f659d5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(path=\"qdrant_mm_db_llama_v3\")\n",
    "\n",
    "llama_text_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"text_collection\"\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=llama_text_store)\n",
    "\n",
    "# Create the Text Vector index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    text_docs,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3678eb02-645a-4354-b431-b8831f9ec3df",
   "metadata": {},
   "source": [
    "## Build Top k retrieval for Vector Store Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a021b5a-450b-4c6d-9589-36eec4397e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 50\n",
    "retriever_engine = index.as_retriever(\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "# retrieve more information from the GPT4V response\n",
    "retrieval_results = retriever_engine.retrieve(\"Compare llama2 with llama1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b31b42-01dd-416c-9fdc-529730d10f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    display_source_node(res_node, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190fd47c-15d7-4986-a80f-215a691cc01b",
   "metadata": {},
   "source": [
    "## Perform query engine on the index and answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f858e8-5e96-460c-a6ae-15158af32657",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query_engine.query(\"Compare llama2 with llama1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4528bac-0bf1-4a99-b417-592690bbf193",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "GPT4V is not stable to identify table and extract table content from image espcially when the image is mixed with tables, texts, and images. It is common in PDF format.\n",
    "\n",
    "By splitting PDF files into single images and let GPT4V understand/summarize each PDF page as an single image, then build RAG based on PDF image to text index. This method is not performing well for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b137b-b82b-4e75-863f-153f62e4388f",
   "metadata": {},
   "source": [
    "# Experiment-3: Letâ€™s use microsoft ``Table Transformer`` to crop tables from the images and see if it gives the correct answer.\n",
    "\n",
    "Thanks to [Neils](https://x.com/NielsRogge). We have modified the utils from the [repository](https://huggingface.co/spaces/nielsr/tatr-demo) for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6009df-934c-41b3-8bea-a11a38300440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxResize(object):\n",
    "    def __init__(self, max_size=800):\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.size\n",
    "        current_max_size = max(width, height)\n",
    "        scale = self.max_size / current_max_size\n",
    "        resized_image = image.resize(\n",
    "            (int(round(scale * width)), int(round(scale * height)))\n",
    "        )\n",
    "\n",
    "        return resized_image\n",
    "\n",
    "\n",
    "detection_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(800),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "structure_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(1000),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load table detection model\n",
    "# processor = TableTransformerImageProcessor(max_size=800)\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-transformer-detection\", revision=\"no_timm\"\n",
    ").to(device)\n",
    "\n",
    "# load table structure recognition model\n",
    "# structure_processor = TableTransformerImageProcessor(max_size=1000)\n",
    "structure_model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-transformer-structure-recognition-v1.1-all\"\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    width, height = size\n",
    "    boxes = box_cxcywh_to_xyxy(out_bbox)\n",
    "    boxes = boxes * torch.tensor(\n",
    "        [width, height, width, height], dtype=torch.float32\n",
    "    )\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n",
    "    pred_bboxes = [\n",
    "        elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)\n",
    "    ]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if not class_label == \"no object\":\n",
    "            objects.append(\n",
    "                {\n",
    "                    \"label\": class_label,\n",
    "                    \"score\": float(score),\n",
    "                    \"bbox\": [float(elem) for elem in bbox],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return objects\n",
    "\n",
    "\n",
    "def detect_and_crop_save_table(\n",
    "    file_path, cropped_table_directory=\"./table_images/\"\n",
    "):\n",
    "    image = Image.open(file_path)\n",
    "\n",
    "    filename, _ = os.path.splitext(file_path.split(\"/\")[-1])\n",
    "\n",
    "    if not os.path.exists(cropped_table_directory):\n",
    "        os.makedirs(cropped_table_directory)\n",
    "\n",
    "    # prepare image for the model\n",
    "    # pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = detection_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "\n",
    "    # postprocess to get detected tables\n",
    "    id2label = model.config.id2label\n",
    "    id2label[len(model.config.id2label)] = \"no object\"\n",
    "    detected_tables = outputs_to_objects(outputs, image.size, id2label)\n",
    "\n",
    "    print(f\"number of tables detected {len(detected_tables)}\")\n",
    "\n",
    "    for idx in range(len(detected_tables)):\n",
    "        #   # crop detected table out of image\n",
    "        cropped_table = image.crop(detected_tables[idx][\"bbox\"])\n",
    "        cropped_table.save(f\"./{cropped_table_directory}/{filename}_{idx}.png\")\n",
    "\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d8107-5cec-4ca0-ae99-d3f493280964",
   "metadata": {},
   "source": [
    "Crop the tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e58814-a86b-4fa3-9c4f-dfec66cb0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in retrieved_images:\n",
    "    detect_and_crop_save_table(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66cd09-a2a5-4b7e-ad18-512eb4168f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cropped tables\n",
    "image_documents = SimpleDirectoryReader(\"./table_images/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee777c-ef42-4ebb-8254-a7903de6deca",
   "metadata": {},
   "source": [
    "## Generate response for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e644d-07ac-4d12-8a08-1ce67b8e124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_mm_llm.complete(\n",
    "    prompt=\"Compare llama2 with llama1?\",\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0bf3b-529a-4ca8-bc6d-54b38b196f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "table_images_paths = glob.glob(\"./table_images/*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf31e4-5fab-44ca-a9f1-c456acfd44f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(table_images_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30976a6f-2f7b-4cea-903f-f44f1a6c0cdc",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "As demonstrated, the model now provides accurate answers. This aligns with our findings from the Chain of Thought (COT) experiments, where supplying GPT-4-V with specific image information significantly enhances its ability to deliver correct responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e489152c-f76c-4353-9a8e-baaabb255218",
   "metadata": {},
   "source": [
    "# Experiment-4: Applying OCR on cropped table images and send the data to GPT4/ GPT-3.5 to answer the query.\n",
    "\n",
    "The experiment depends highly on the OCR model used. Here we are using easyocr with few modifications from [repository](https://huggingface.co/spaces/nielsr/tatr-demo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b0f60-93fd-4834-88a6-d7d204ffdd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "\n",
    "reader = easyocr.Reader([\"en\"])\n",
    "\n",
    "\n",
    "def detect_and_crop_table(image):\n",
    "    # prepare image for the model\n",
    "    # pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = detection_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "\n",
    "    # postprocess to get detected tables\n",
    "    id2label = model.config.id2label\n",
    "    id2label[len(model.config.id2label)] = \"no object\"\n",
    "    detected_tables = outputs_to_objects(outputs, image.size, id2label)\n",
    "\n",
    "    # visualize\n",
    "    # fig = visualize_detected_tables(image, detected_tables)\n",
    "    # image = fig2img(fig)\n",
    "\n",
    "    # crop first detected table out of image\n",
    "    cropped_table = image.crop(detected_tables[0][\"bbox\"])\n",
    "\n",
    "    return cropped_table\n",
    "\n",
    "\n",
    "def recognize_table(image):\n",
    "    # prepare image for the model\n",
    "    # pixel_values = structure_processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = structure_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = structure_model(pixel_values)\n",
    "\n",
    "    # postprocess to get individual elements\n",
    "    id2label = structure_model.config.id2label\n",
    "    id2label[len(structure_model.config.id2label)] = \"no object\"\n",
    "    cells = outputs_to_objects(outputs, image.size, id2label)\n",
    "\n",
    "    # visualize cells on cropped table\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for cell in cells:\n",
    "        draw.rectangle(cell[\"bbox\"], outline=\"red\")\n",
    "\n",
    "    return image, cells\n",
    "\n",
    "\n",
    "def get_cell_coordinates_by_row(table_data):\n",
    "    # Extract rows and columns\n",
    "    rows = [entry for entry in table_data if entry[\"label\"] == \"table row\"]\n",
    "    columns = [\n",
    "        entry for entry in table_data if entry[\"label\"] == \"table column\"\n",
    "    ]\n",
    "\n",
    "    # Sort rows and columns by their Y and X coordinates, respectively\n",
    "    rows.sort(key=lambda x: x[\"bbox\"][1])\n",
    "    columns.sort(key=lambda x: x[\"bbox\"][0])\n",
    "\n",
    "    # Function to find cell coordinates\n",
    "    def find_cell_coordinates(row, column):\n",
    "        cell_bbox = [\n",
    "            column[\"bbox\"][0],\n",
    "            row[\"bbox\"][1],\n",
    "            column[\"bbox\"][2],\n",
    "            row[\"bbox\"][3],\n",
    "        ]\n",
    "        return cell_bbox\n",
    "\n",
    "    # Generate cell coordinates and count cells in each row\n",
    "    cell_coordinates = []\n",
    "\n",
    "    for row in rows:\n",
    "        row_cells = []\n",
    "        for column in columns:\n",
    "            cell_bbox = find_cell_coordinates(row, column)\n",
    "            row_cells.append({\"column\": column[\"bbox\"], \"cell\": cell_bbox})\n",
    "\n",
    "        # Sort cells in the row by X coordinate\n",
    "        row_cells.sort(key=lambda x: x[\"column\"][0])\n",
    "\n",
    "        # Append row information to cell_coordinates\n",
    "        cell_coordinates.append(\n",
    "            {\n",
    "                \"row\": row[\"bbox\"],\n",
    "                \"cells\": row_cells,\n",
    "                \"cell_count\": len(row_cells),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort rows from top to bottom\n",
    "    cell_coordinates.sort(key=lambda x: x[\"row\"][1])\n",
    "\n",
    "    return cell_coordinates\n",
    "\n",
    "\n",
    "def apply_ocr(cell_coordinates, cropped_table):\n",
    "    # let's OCR row by row\n",
    "    data = dict()\n",
    "    max_num_columns = 0\n",
    "    for idx, row in enumerate(cell_coordinates):\n",
    "        row_text = []\n",
    "        for cell in row[\"cells\"]:\n",
    "            # crop cell out of image\n",
    "            cell_image = np.array(cropped_table.crop(cell[\"cell\"]))\n",
    "            # apply OCR\n",
    "            result = reader.readtext(np.array(cell_image))\n",
    "            if len(result) > 0:\n",
    "                text = \" \".join([x[1] for x in result])\n",
    "                row_text.append(text)\n",
    "\n",
    "        if len(row_text) > max_num_columns:\n",
    "            max_num_columns = len(row_text)\n",
    "\n",
    "        data[str(idx)] = row_text\n",
    "\n",
    "    # pad rows which don't have max_num_columns elements\n",
    "    # to make sure all rows have the same number of columns\n",
    "    for idx, row_data in data.copy().items():\n",
    "        if len(row_data) != max_num_columns:\n",
    "            row_data = row_data + [\n",
    "                \"\" for _ in range(max_num_columns - len(row_data))\n",
    "            ]\n",
    "        data[str(idx)] = row_data\n",
    "\n",
    "    text = \", \".join(f\"{key}={value}\" for key, value in data.items())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a7ddf3-fb4a-4755-b097-bfa3d50a0c13",
   "metadata": {},
   "source": [
    "Extract table information from the table images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0597f7e-6210-44cb-98e0-78efcf0341e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_text = \"\"\n",
    "\n",
    "for table_image in table_images_paths:\n",
    "    try:\n",
    "        cropped_table = Image.open(table_image)\n",
    "        image, cells = recognize_table(cropped_table)\n",
    "\n",
    "        cell_coordinates = get_cell_coordinates_by_row(cells)\n",
    "\n",
    "        text = apply_ocr(cell_coordinates, image)\n",
    "\n",
    "        table_text = table_text + text + \"\\n\"\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20474c1f-d56a-4aeb-b012-06f41fb5a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760da030-79a8-4627-9468-1e937710ce54",
   "metadata": {},
   "source": [
    "As you can see the table extracted is not very accurate. (Each row represents a table information)\n",
    "\n",
    "Letâ€™s now send it LLM to answer our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1255cf-0640-4f28-a307-bf21113692e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "query = f\"\"\"Based on the following table information extracted, answer the query: \\n\n",
    "\n",
    "            TABLE INFORMATION:\n",
    "\n",
    "            {table_text}\n",
    "\n",
    "\n",
    "            Query:\n",
    "\n",
    "            Compare llama2 with llama1?\n",
    "            \"\"\"\n",
    "response = llm.complete(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82eaa1-dfe6-4b73-b721-5810886d13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e1f7da-0ac6-4864-8b64-55571ab43b89",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "Because we could not extract the table information from image, the answer is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f418afe-be99-4974-b23e-3955c2b15b07",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we have explored different ways of handling table information present in PDFâ€™s. Specifically we explored Microsoft ``Table Transformer`` to crop tables from images and process it to get accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5a9c7-088c-4105-9309-f34d2608bc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
