{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59df23c-86f7-4e5d-8b8c-de92a92f6637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "# Extract elements from PDF\n",
    "def extract_pdf_elements(path, fname):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    return partition_pdf(fname,\n",
    "                       extract_images_in_pdf=True,\n",
    "                       infer_table_structure=True,\n",
    "                       strategy=\"auto\",\n",
    "                        max_characters=4000,\n",
    "                        new_after_n_chars=3800,\n",
    "                        combine_text_under_n_chars=2000\n",
    "    )\n",
    "\n",
    "\n",
    "# Categorize elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "# File path\n",
    "fpath = \"/project/code/Attention_is_all_you_need.pdf\"\n",
    "fname = \"Attention_is_all_you_need.pdf\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = extract_pdf_elements(fpath, fname)\n",
    "\n",
    "# Get text, tables\n",
    "texts, tables = categorize_elements(raw_pdf_elements)\n",
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523e6ed2-2132-4748-bdb7-db765f20648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6b1d97-4245-45ac-95ba-9bc1cfd10182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 2/2 [00:00<00:00, 45590.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=200)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the raw image. \\\n",
    "        Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for img_file in tqdm(sorted(os.listdir(path)), desc=\"Processing images\"):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "\n",
    "            try:\n",
    "                base64_image = encode_image(img_path)\n",
    "                img_base64_list.append(base64_image)\n",
    "                image_summaries.append(image_summarize(base64_image, prompt))\n",
    "                count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_file}: {e}\")\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(\"/project/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a0a289-b970-49fe-b04f-5d857a4c159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2318/2114994243.py:53: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "import uuid, os\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag-storage\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "771a47fa-1267-4db8-a6ae-5fde48bbc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are a deep learning and machine learning specialist.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts.\\n\"\n",
    "            \"Use this information to provide quality information related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4695c6-7374-4284-b2fe-a94ac17b630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2318/2728129363.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  docs = retriever_multi_vector_img.get_relevant_documents(query, limit=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check retrieval\n",
    "query = \"Can you give me a brief description on the document.\"\n",
    "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=6)\n",
    "\n",
    "# We get 4 docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c64b19e-5a89-4dda-af38-fcc4a36a1b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document appears to be a detailed comparison and analysis of various machine learning models, particularly focusing on neural network architectures and their performance metrics\n",
      " Here's a brief description of the key sections and their contents:\n",
      "\n",
      "1\n",
      " **Model Configuration and Performance Metrics**:\n",
      "   - The table lists different configurations of a base model with parameters such as the number of layers (N), model dimension (dmodel), feed-forward dimension (dff), number of attention heads (h), key dimension (dk), value dimension (dv), dropout rate (Pdrop), label smoothing (ϵls), training steps, perplexity (PPL) on the development set, BLEU score, and the number of parameters (params)\n",
      "\n",
      "   - Different configurations (A, B, C, D, E) are compared based on these metrics, showing how changes in model architecture and hyperparameters affect performance\n",
      "\n",
      "\n",
      "2\n",
      " **BLEU Scores and Training Costs**:\n",
      "   - This section compares the BLEU scores and training costs (in FLOPs) of various models for English-to-German (EN-DE) and English-to-French (EN-FR) translation tasks\n",
      "\n",
      "   - Models compared include ByteNet, Deep-Att + PosUnk, GNMT + RL, ConvS2S, MoE, and different configurations of the Transformer model (base and big)\n",
      "\n",
      "   - The Transformer models, particularly the \"big\" configuration, show competitive BLEU scores with relatively lower training costs\n",
      "\n",
      "\n",
      "3\n",
      " **Parser Training Performance**:\n",
      "   - This section presents the F1 scores of different models on the WSJ (Wall Street Journal) dataset for parsing tasks\n",
      "\n",
      "   - Models include those by Vinyals & Kaiser, Petrov et al\n",
      ", Zhu et al\n",
      ", Dyer et al\n",
      ", and various configurations of the Transformer model\n",
      "\n",
      "   - The Transformer model with 4 layers shows strong performance, both in discriminative and semi-supervised settings, with F1 scores comparable to or better than other models\n",
      "\n",
      "\n",
      "4\n",
      " **Layer Type Complexity**:\n",
      "   - This table compares the complexity per layer, sequential operations, and maximum path length for different types of layers: Self-Attention, Recurrent, Convolutional, and Restricted Self-Attention\n",
      "\n",
      "   - Self-Attention layers have a complexity of \\(O(n^2 \\cdot d)\\) but offer constant sequential operations and path length, making them efficient for parallel processing\n",
      "\n",
      "   - Recurrent layers have a linear complexity \\(O(n \\cdot d^2)\\) but require sequential operations and have a path length proportional to the sequence length\n",
      "\n",
      "   - Convolutional layers have a complexity dependent on the kernel size \\(k\\), with \\(O(k \\cdot n \\cdot d^2)\\), and offer logarithmic path length with respect to the sequence length\n",
      "\n",
      "\n",
      "Overall, the document provides a comprehensive comparison of different neural network models and configurations, highlighting the efficiency and performance of the Transformer model in various tasks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run RAG chain\n",
    "response = chain_multimodal_rag.invoke(query)\n",
    "response = response.split('.')\n",
    "\n",
    "# Print each line in a new line\n",
    "for line in response:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "846ea682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the transformer model architecture, the layer that comes after the Add & Norm layer following the Multi-head Attention layer is the Feed-Forward Neural Network (FFN) layer\n",
      " \n",
      "\n",
      "Here is a brief overview of the sequence of layers in a transformer model:\n",
      "\n",
      "1\n",
      " **Multi-head Self-Attention**: This layer allows the model to focus on different parts of the input sequence simultaneously\n",
      "\n",
      "2\n",
      " **Add & Norm**: The output of the Multi-head Self-Attention layer is added to the original input (residual connection) and then normalized\n",
      "\n",
      "3\n",
      " **Feed-Forward Neural Network (FFN)**: This layer consists of two linear transformations with a ReLU activation in between\n",
      " It is applied to each position separately and identically\n",
      "\n",
      "\n",
      "The sequence can be summarized as follows:\n",
      "- Multi-head Self-Attention\n",
      "- Add & Norm\n",
      "- Feed-Forward Neural Network (FFN)\n",
      "- Add & Norm\n",
      "\n",
      "This pattern is repeated in each layer of the transformer model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What layer comes after the Add & Norm of the Multi-head Attention mechanism of the transformer model architecture?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e45a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of the scaled dot-product attention mechanism, the layer that typically follows the optional Mask layer is the **Softmax layer**\n",
      " \n",
      "\n",
      "Here's a brief overview of the process:\n",
      "\n",
      "1\n",
      " **Query, Key, and Value Matrices**: The input is transformed into three matrices: Query (Q), Key (K), and Value (V)\n",
      "\n",
      "2\n",
      " **Scaled Dot-Product**: The dot product of the Query and Key matrices is computed and scaled by the square root of the dimension of the Key matrix\n",
      "\n",
      "3\n",
      " **Optional Masking**: A mask can be applied to the scaled dot-product to prevent attending to certain positions (e\n",
      "g\n",
      ", future positions in the case of decoder self-attention in transformers)\n",
      "\n",
      "4\n",
      " **Softmax**: The result of the scaled dot-product (after optional masking) is passed through a Softmax layer to obtain the attention weights\n",
      "\n",
      "5\n",
      " **Weighted Sum**: The attention weights are then used to compute a weighted sum of the Value matrix\n",
      "\n",
      "\n",
      "So, the sequence of operations in the scaled dot-product attention block is:\n",
      "\n",
      "1\n",
      " Compute \\( QK^T / \\sqrt{d_k} \\)\n",
      "2\n",
      " Apply optional mask\n",
      "3\n",
      " Apply **Softmax**\n",
      "4\n",
      " Compute weighted sum with the Value matrix \\( V \\)\n",
      "\n",
      "Therefore, the layer that comes after the optional Mask layer is the **Softmax layer**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What layer comes after the optional Mask layer in the scaled dot-product attention block?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e7ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score for the ByteNet model, as provided in the text, is 23\n",
      "75 for the EN-DE (English to German) translation task\n",
      " The table does not provide a BLEU score for the EN-FR (English to French) translation task for the ByteNet model\n",
      " \n",
      "\n",
      "Here is the relevant excerpt from the table for clarity:\n",
      "\n",
      "```\n",
      "Model          BLEU EN-DE  EN-FR\n",
      "ByteNet [18]   23\n",
      "75\n",
      "```\n",
      "\n",
      "This indicates that the ByteNet model achieved a BLEU score of 23\n",
      "75 on the English to German translation task\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What is the BLEU score for ByteNet model?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74c9223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score for the ConvS2S (Convolutional Sequence to Sequence) model varies depending on the language pair and whether an ensemble model is used\n",
      " Based on the provided data:\n",
      "\n",
      "- For the English-to-German (EN-DE) translation task:\n",
      "  - The ConvS2S model achieves a BLEU score of **25\n",
      "16**\n",
      "\n",
      "  - The ConvS2S Ensemble model achieves a BLEU score of **26\n",
      "36**\n",
      "\n",
      "\n",
      "- For the English-to-French (EN-FR) translation task:\n",
      "  - The ConvS2S model achieves a BLEU score of **40\n",
      "46**\n",
      "\n",
      "  - The ConvS2S Ensemble model achieves a BLEU score of **41\n",
      "29**\n",
      "\n",
      "\n",
      "These scores indicate the performance of the ConvS2S model in terms of translation quality, with the ensemble models generally performing better than the single models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What is the BLEU score for ConvS2S model?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5712dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The complexity of the Maximum Path Lengths for a convolutional layer type can be derived from the provided table\n",
      " Let's break down the information:\n",
      "\n",
      "### Complexity of Maximum Path Lengths for Different Layer Types\n",
      "\n",
      "| Layer Type                | Complexity per Layer | Sequential Maximum Path Length | Operations |\n",
      "|---------------------------|----------------------|--------------------------------|------------|\n",
      "| Self-Attention            | O(n² · d)            | O(1)                           | O(1)       |\n",
      "| Recurrent                 | O(n · d²)            | O(n)                           | O(n)       |\n",
      "| Convolutional             | O(k · n · d²)        | O(1)                           | O(logₖ(n)) |\n",
      "| Self-Attention (restricted)| O(r · n · d)         | O(1)                           | O(n/r)     |\n",
      "\n",
      "### Explanation\n",
      "\n",
      "- **Layer Type**: This column lists the type of neural network layer\n",
      "\n",
      "- **Complexity per Layer**: This column shows the computational complexity for each layer type\n",
      "\n",
      "- **Sequential Maximum Path Length**: This column indicates the maximum path length in a sequential manner, which is crucial for understanding the depth of the network and how information flows through it\n",
      "\n",
      "- **Operations**: This column provides the complexity of operations required for each layer type\n",
      "\n",
      "\n",
      "### Convolutional Layer\n",
      "\n",
      "For a convolutional layer:\n",
      "- **Complexity per Layer**: \\( O(k \\cdot n \\cdot d^2) \\)\n",
      "  - \\( k \\): Kernel size\n",
      "  - \\( n \\): Sequence length\n",
      "  - \\( d \\): Dimensionality of the model\n",
      "- **Sequential Maximum Path Length**: \\( O(1) \\)\n",
      "  - This indicates that the maximum path length is constant and does not depend on the sequence length \\( n \\)\n",
      "\n",
      "- **Operations**: \\( O(\\log_k(n)) \\)\n",
      "  - This shows that the number of operations grows logarithmically with the sequence length \\( n \\) and inversely with the kernel size \\( k \\)\n",
      "\n",
      "\n",
      "### Summary\n",
      "\n",
      "The convolutional layer has a constant sequential maximum path length of \\( O(1) \\), meaning that the depth of the network in terms of the number of layers does not increase with the sequence length\n",
      " This is advantageous for parallel processing and can lead to faster training times compared to recurrent layers, which have a sequential maximum path length of \\( O(n) \\)\n",
      " The operations complexity of \\( O(\\log_k(n)) \\) indicates that the number of operations required grows logarithmically with the sequence length, making convolutional layers efficient for handling long sequences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What is the complexity of the Maximum Path Lengths for a convolutional layer type?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d0f4f-fad3-4b84-af9e-dcaa0ae39ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
