{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59df23c-86f7-4e5d-8b8c-de92a92f6637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "# Extract elements from PDF\n",
    "def extract_pdf_elements(path, fname):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    return partition_pdf(fname,\n",
    "                       extract_images_in_pdf=True,\n",
    "                       infer_table_structure=True,\n",
    "                       strategy=\"auto\",\n",
    "                        max_characters=4000,\n",
    "                        new_after_n_chars=3800,\n",
    "                        combine_text_under_n_chars=2000\n",
    "    )\n",
    "\n",
    "\n",
    "# Categorize elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "# File path\n",
    "fpath = \"/project/code/Attention_is_all_you_need.pdf\"\n",
    "fname = \"Attention_is_all_you_need.pdf\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = extract_pdf_elements(fpath, fname)\n",
    "\n",
    "# Get text, tables\n",
    "texts, tables = categorize_elements(raw_pdf_elements)\n",
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523e6ed2-2132-4748-bdb7-db765f20648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6b1d97-4245-45ac-95ba-9bc1cfd10182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 2/2 [00:00<00:00, 32513.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=200)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the raw image. \\\n",
    "        Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for img_file in tqdm(sorted(os.listdir(path)), desc=\"Processing images\"):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "\n",
    "            try:\n",
    "                base64_image = encode_image(img_path)\n",
    "                img_base64_list.append(base64_image)\n",
    "                image_summaries.append(image_summarize(base64_image, prompt))\n",
    "                count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_file}: {e}\")\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(\"/project/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a0a289-b970-49fe-b04f-5d857a4c159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_238/2114994243.py:53: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "import uuid, os\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag-storage\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "771a47fa-1267-4db8-a6ae-5fde48bbc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are a deep learning and machine learning specialist.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts.\\n\"\n",
    "            \"Use this information to provide quality information related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4695c6-7374-4284-b2fe-a94ac17b630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_238/2728129363.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  docs = retriever_multi_vector_img.get_relevant_documents(query, limit=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check retrieval\n",
    "query = \"Can you give me a brief description on the document.\"\n",
    "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=6)\n",
    "\n",
    "# We get 4 docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c64b19e-5a89-4dda-af38-fcc4a36a1b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document appears to be a detailed comparison and analysis of various machine learning models, particularly focusing on the Transformer model and its variants\n",
      " Here is a brief description of the key points covered:\n",
      "\n",
      "1\n",
      " **Transformer Model Configurations and Performance**:\n",
      "   - The table lists different configurations of the Transformer model, including parameters like the number of layers (N), model dimension (dmodel), feed-forward dimension (dff), number of attention heads (h), and others\n",
      "\n",
      "   - Performance metrics such as Perplexity (PPL) on the development set and BLEU scores are provided for different configurations\n",
      "\n",
      "   - The base model configuration is highlighted, and several other configurations (A, B, C, D, E) are compared in terms of their performance\n",
      "\n",
      "\n",
      "2\n",
      " **Comparison with Other Models**:\n",
      "   - The document compares the BLEU scores and training costs (in FLOPs) of the Transformer model with other models like ByteNet, GNMT + RL, ConvS2S, and MoE\n",
      "\n",
      "   - Both single models and ensemble models are considered, showing that the Transformer (big) model achieves the highest BLEU scores for EN-DE and EN-FR translations\n",
      "\n",
      "\n",
      "3\n",
      " **Parser Training Performance**:\n",
      "   - The performance of different parsing models on the WSJ dataset is compared using the F1 score\n",
      "\n",
      "   - The Transformer model with 4 layers is shown to perform competitively with other state-of-the-art models, both in discriminative and semi-supervised settings\n",
      "\n",
      "\n",
      "4\n",
      " **Complexity Analysis**:\n",
      "   - The document includes a table comparing the complexity per layer, sequential operations, and maximum path length for different layer types: Self-Attention, Recurrent, Convolutional, and Restricted Self-Attention\n",
      "\n",
      "   - Self-Attention layers are noted for their efficiency in terms of sequential operations and maximum path length\n",
      "\n",
      "\n",
      "Overall, the document provides a comprehensive analysis of the Transformer model's performance and efficiency compared to other models, highlighting its strengths in various configurations and tasks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run RAG chain\n",
    "response = chain_multimodal_rag.invoke(query)\n",
    "response = response.split('.')\n",
    "\n",
    "# Print each line in a new line\n",
    "for line in response:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "846ea682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention mechanism is a fundamental concept in modern deep learning, particularly in the context of natural language processing (NLP) and sequence modeling\n",
      " It allows models to focus on specific parts of the input sequence when making predictions, which significantly improves performance in tasks like machine translation, text summarization, and more\n",
      " Here's a detailed explanation based on the provided information:\n",
      "\n",
      "### What is the Attention Mechanism?\n",
      "\n",
      "The attention mechanism was introduced to address the limitations of traditional sequence models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs)\n",
      " These traditional models often struggle with long-range dependencies due to their sequential nature\n",
      " Attention mechanisms, on the other hand, allow the model to weigh the importance of different parts of the input sequence dynamically\n",
      "\n",
      "\n",
      "### Key Components of Attention Mechanism\n",
      "\n",
      "1\n",
      " **Query, Key, and Value Vectors**:\n",
      "   - **Query (Q)**: Represents the current word or token for which the attention is being calculated\n",
      "\n",
      "   - **Key (K)**: Represents all words or tokens in the input sequence\n",
      "\n",
      "   - **Value (V)**: Represents the actual values or embeddings of the words or tokens in the input sequence\n",
      "\n",
      "\n",
      "2\n",
      " **Attention Score Calculation**:\n",
      "   - The attention score is calculated using a similarity measure (often dot product) between the query and key vectors\n",
      " This score determines how much focus the model should place on each part of the input sequence\n",
      "\n",
      "\n",
      "3\n",
      " **Softmax Function**:\n",
      "   - The attention scores are passed through a softmax function to obtain a probability distribution\n",
      " This ensures that the scores sum up to 1, making it easier to interpret them as weights\n",
      "\n",
      "\n",
      "4\n",
      " **Weighted Sum**:\n",
      "   - The final attention output is a weighted sum of the value vectors, where the weights are the attention scores obtained from the softmax function\n",
      "\n",
      "\n",
      "### Types of Attention Mechanisms\n",
      "\n",
      "1\n",
      " **Self-Attention**:\n",
      "   - Self-attention, also known as intra-attention, is a mechanism where the model attends to different positions of the same input sequence\n",
      " This is particularly useful in models like the Transformer, which relies heavily on self-attention to capture dependencies between words in a sentence\n",
      "\n",
      "\n",
      "2\n",
      " **Global vs\n",
      " Local Attention**:\n",
      "   - **Global Attention**: Considers all positions in the input sequence\n",
      "\n",
      "   - **Local Attention**: Focuses on a subset of the input sequence, which can reduce computational complexity\n",
      "\n",
      "\n",
      "### Computational Complexity\n",
      "\n",
      "The provided table compares the computational complexity of different layer types, including self-attention, recurrent, and convolutional layers:\n",
      "\n",
      "- **Self-Attention**: \\(O(n^2 \\cdot d)\\)\n",
      "- **Recurrent**: \\(O(n \\cdot d^2)\\)\n",
      "- **Convolutional**: \\(O(k \\cdot n \\cdot d^2)\\)\n",
      "- **Restricted Self-Attention**: \\(O(r \\cdot n \\cdot d)\\)\n",
      "\n",
      "Here, \\(n\\) is the sequence length, \\(d\\) is the dimensionality of the model, \\(k\\) is the kernel size, and \\(r\\) is the restriction factor\n",
      "\n",
      "\n",
      "### Performance Metrics\n",
      "\n",
      "The tables also provide performance metrics for various models using attention mechanisms:\n",
      "\n",
      "- **Parser Training**: The Transformer model with 4 layers achieves a high F1 score, indicating its effectiveness in parsing tasks\n",
      "\n",
      "- **BLEU Scores**: The Transformer models (base and big) achieve high BLEU scores in machine translation tasks, demonstrating the effectiveness of attention mechanisms in improving translation quality\n",
      "\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "The attention mechanism is a powerful tool in deep learning that allows models to dynamically focus on relevant parts of the input sequence\n",
      " It has revolutionized NLP and sequence modeling, leading to significant improvements in various tasks\n",
      " The computational complexity and performance metrics provided highlight the efficiency and effectiveness of attention-based models like the Transformer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What is attetion mechanism?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e45a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset used for training in the provided information is the Wall Street Journal (WSJ) dataset\n",
      " This is evident from multiple references to \"WSJ only\" in the context of discriminative training methods and semi-supervised training methods\n",
      " \n",
      "\n",
      "Here are the specific instances where the WSJ dataset is mentioned:\n",
      "\n",
      "1\n",
      " **Discriminative Training:**\n",
      "   - Vinyals & Kaiser et al\n",
      " (2014) [37]: WSJ only, discriminative\n",
      "   - Petrov et al\n",
      " (2006) [29]: WSJ only, discriminative\n",
      "   - Zhu et al\n",
      " (2013) [40]: WSJ only, discriminative\n",
      "   - Dyer et al\n",
      " (2016) [8]: WSJ only, discriminative\n",
      "   - Transformer (4 layers): WSJ only, discriminative\n",
      "\n",
      "2\n",
      " **Semi-Supervised Training:**\n",
      "   - Zhu et al\n",
      " (2013) [40]: semi-supervised\n",
      "   - Huang & Harper (2009) [14]: semi-supervised\n",
      "   - McClosky et al\n",
      " (2006) [26]: semi-supervised\n",
      "   - Vinyals & Kaiser et al\n",
      " (2014) [37]: semi-supervised\n",
      "   - Transformer (4 layers): semi-supervised\n",
      "\n",
      "The WSJ dataset is a well-known dataset in the field of natural language processing and is often used for training and evaluating parsing models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What dataset was for used for training?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e7ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score for the ByteNet model, as provided in the text, is 23\n",
      "75\n",
      " This score is specifically for the English-to-German (EN-DE) translation task\n",
      " The table does not provide a BLEU score for ByteNet on the English-to-French (EN-FR) translation task\n",
      " \n",
      "\n",
      "Here is the relevant excerpt from the table for clarity:\n",
      "\n",
      "```\n",
      "Model          BLEU EN-DE  EN-FR\n",
      "ByteNet [18]   23\n",
      "75\n",
      "```\n",
      "\n",
      "This indicates that ByteNet achieved a BLEU score of 23\n",
      "75 for translating from English to German\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What is the BLEU score for ByteNet model?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74c9223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score for the ConvS2S (Convolutional Sequence to Sequence) model varies depending on the language pair being translated\n",
      " Based on the provided data:\n",
      "\n",
      "- For English to German (EN-DE), the ConvS2S model achieves a BLEU score of **25\n",
      "16**\n",
      "\n",
      "- For English to French (EN-FR), the ConvS2S model achieves a BLEU score of **40\n",
      "46**\n",
      "\n",
      "\n",
      "Additionally, when using an ensemble of ConvS2S models, the BLEU scores improve:\n",
      "\n",
      "- For English to German (EN-DE), the ConvS2S Ensemble achieves a BLEU score of **26\n",
      "36**\n",
      "\n",
      "- For English to French (EN-FR), the ConvS2S Ensemble achieves a BLEU score of **41\n",
      "29**\n",
      "\n",
      "\n",
      "These scores indicate the performance of the ConvS2S model in terms of translation quality for the specified language pairs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What is the BLEU score for ConvS2S model?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5712dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The part of the document that contains all the BLEU scores for different models is the following table:\n",
      "\n",
      "```\n",
      "Model BLEU EN-DE EN-FR Training Cost (FLOPs) EN-DE EN-FR\n",
      "ByteNet [18] 23\n",
      "75 \n",
      "Deep-Att + PosUnk [39] 39\n",
      "2 1\n",
      "0 · 1020 \n",
      "GNMT + RL [38] 24\n",
      "6 39\n",
      "92 2\n",
      "3 · 1019 1\n",
      "4 · 1020 \n",
      "ConvS2S [9] 25\n",
      "16 40\n",
      "46 9\n",
      "6 · 1018 1\n",
      "5 · 1020 \n",
      "MoE [32] 26\n",
      "03 40\n",
      "56 2\n",
      "0 · 1019 1\n",
      "2 · 1020 \n",
      "Deep-Att + PosUnk Ensemble [39] 40\n",
      "4 8\n",
      "0 · 1020 \n",
      "GNMT + RL Ensemble [38] 26\n",
      "30 41\n",
      "16 1\n",
      "8 · 1020 1\n",
      "1 · 1021 \n",
      "ConvS2S Ensemble [9] 26\n",
      "36 41\n",
      "29 7\n",
      "7 · 1019 1\n",
      "2 · 1021 \n",
      "Transformer (base model) 27\n",
      "3 38\n",
      "1 3\n",
      "3 · 1018 \n",
      "Transformer (big) 28\n",
      "4 41\n",
      "8 2\n",
      "3 · 1019\n",
      "```\n",
      "\n",
      "This table lists the BLEU scores for different models for English-to-German (EN-DE) and English-to-French (EN-FR) translations, along with their respective training costs in FLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_multimodal_rag.invoke(\"What part of the documnet has all the BLEU score for different models?\")\n",
    "response = response.split(\".\")\n",
    "for line in response:\n",
    "    print (line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
